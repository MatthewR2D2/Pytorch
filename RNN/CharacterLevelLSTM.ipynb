{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data.txt\", 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([32,  7, 35,  5,  0, 21, 54, 15, 11, 42, 42, 42, 31, 35,  5,  5, 59,\n",
       "       15, 38, 35, 79, 65, 45, 65, 21,  6, 15, 35, 54, 21, 15, 35, 45, 45,\n",
       "       15, 35, 45, 65,  1, 21, 62, 15, 21, 25, 21, 54, 59, 15, 70, 46,  7,\n",
       "       35,  5,  5, 59, 15, 38, 35, 79, 65, 45, 59, 15, 65,  6, 15, 70, 46,\n",
       "        7, 35,  5,  5, 59, 15, 65, 46, 15, 65,  0,  6, 15, 51, 50, 46, 42,\n",
       "       50, 35, 59, 56, 42, 42, 27, 25, 21, 54, 59,  0,  7, 65, 46])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TOKENIZATION\n",
    "# encode the text and map each character to an integer and vice versa\n",
    "# 1. int2char, which maps integers to characters\n",
    "# 2. char2int, which maps characters to unique integers\n",
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "# encode the text\n",
    "encoded = np.array([char2int[ch] for ch in text])\n",
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def one_hot_encode(arr, n_labels):\n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "# check that the function works as expected\n",
    "test_seq = np.array([[3, 5, 1]])\n",
    "one_hot = one_hot_encode(test_seq, 8)\n",
    "\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_batches(arr, batch_size, seq_length):\n",
    "    '''Create a generator that returns batches of size\n",
    "       batch_size x seq_length from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       batch_size: Batch size, the number of sequences per batch\n",
    "       seq_length: Number of encoded chars in a sequence\n",
    "    '''\n",
    "    \n",
    "    batch_size_total = batch_size * seq_length\n",
    "    # total number of batches we can make\n",
    "    n_batches = len(arr)//batch_size_total\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size_total]\n",
    "    # Reshape into batch_size rows\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    \n",
    "    # iterate through the array, one sequence at a time\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        # The features\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[32  7 35  5  0 21 54 15 11 42]\n",
      " [ 6 51 46 15  0  7 35  0 15 35]\n",
      " [21 46 13 15 51 54 15 35 15 38]\n",
      " [ 6 15  0  7 21 15 75  7 65 21]\n",
      " [15  6 35 50 15  7 21 54 15  0]\n",
      " [75 70  6  6 65 51 46 15 35 46]\n",
      " [15 58 46 46 35 15  7 35 13 15]\n",
      " [80 44 45 51 46  6  1 59 56 15]]\n",
      "\n",
      "y\n",
      " [[ 7 35  5  0 21 54 15 11 42 42]\n",
      " [51 46 15  0  7 35  0 15 35  0]\n",
      " [46 13 15 51 54 15 35 15 38 51]\n",
      " [15  0  7 21 15 75  7 65 21 38]\n",
      " [ 6 35 50 15  7 21 54 15  0 21]\n",
      " [70  6  6 65 51 46 15 35 46 13]\n",
      " [58 46 46 35 15  7 35 13 15  6]\n",
      " [44 45 51 46  6  1 59 56 15 39]]\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(encoded, 8, 50)\n",
    "x, y = next(batches)\n",
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, tokens, n_hidden=256, n_layers=2, drop_prob =0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        # LSTM\n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "        out = self.dropout(r_output)\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define and print the net\n",
    "n_hidden=512\n",
    "n_layers=2\n",
    "\n",
    "net = CharRNN(chars, n_hidden, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "seq_length = 100\n",
    "epochs = 5\n",
    "lr = 0.001\n",
    "val_frac = 0.1\n",
    "clip=5\n",
    "print_every=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5... Step: 10... Loss: 3.2640... Val Loss: 3.1859\n",
      "Epoch: 1/5... Step: 20... Loss: 3.1517... Val Loss: 3.1286\n",
      "Epoch: 1/5... Step: 30... Loss: 3.1476... Val Loss: 3.1204\n",
      "Epoch: 1/5... Step: 40... Loss: 3.1118... Val Loss: 3.1195\n",
      "Epoch: 1/5... Step: 50... Loss: 3.1452... Val Loss: 3.1182\n",
      "Epoch: 1/5... Step: 60... Loss: 3.1214... Val Loss: 3.1154\n",
      "Epoch: 1/5... Step: 70... Loss: 3.1071... Val Loss: 3.1138\n",
      "Epoch: 1/5... Step: 80... Loss: 3.1215... Val Loss: 3.1095\n",
      "Epoch: 1/5... Step: 90... Loss: 3.1164... Val Loss: 3.0991\n",
      "Epoch: 1/5... Step: 100... Loss: 3.0867... Val Loss: 3.0748\n",
      "Epoch: 1/5... Step: 110... Loss: 3.0310... Val Loss: 3.0079\n",
      "Epoch: 1/5... Step: 120... Loss: 2.9134... Val Loss: 2.9153\n",
      "Epoch: 1/5... Step: 130... Loss: 2.8668... Val Loss: 2.8378\n",
      "Epoch: 2/5... Step: 140... Loss: 2.8028... Val Loss: 2.7482\n",
      "Epoch: 2/5... Step: 150... Loss: 2.6989... Val Loss: 2.6615\n",
      "Epoch: 2/5... Step: 160... Loss: 2.6058... Val Loss: 2.5726\n",
      "Epoch: 2/5... Step: 170... Loss: 2.5309... Val Loss: 2.5085\n",
      "Epoch: 2/5... Step: 180... Loss: 2.5006... Val Loss: 2.4706\n",
      "Epoch: 2/5... Step: 190... Loss: 2.4333... Val Loss: 2.4281\n",
      "Epoch: 2/5... Step: 200... Loss: 2.4331... Val Loss: 2.3992\n",
      "Epoch: 2/5... Step: 210... Loss: 2.3900... Val Loss: 2.3723\n",
      "Epoch: 2/5... Step: 220... Loss: 2.3583... Val Loss: 2.3444\n",
      "Epoch: 2/5... Step: 230... Loss: 2.3518... Val Loss: 2.3177\n",
      "Epoch: 2/5... Step: 240... Loss: 2.3203... Val Loss: 2.2873\n",
      "Epoch: 2/5... Step: 250... Loss: 2.2673... Val Loss: 2.2646\n",
      "Epoch: 2/5... Step: 260... Loss: 2.2453... Val Loss: 2.2417\n",
      "Epoch: 2/5... Step: 270... Loss: 2.2852... Val Loss: 2.2962\n",
      "Epoch: 3/5... Step: 280... Loss: 2.2765... Val Loss: 2.2257\n",
      "Epoch: 3/5... Step: 290... Loss: 2.2308... Val Loss: 2.1907\n",
      "Epoch: 3/5... Step: 300... Loss: 2.2079... Val Loss: 2.1715\n",
      "Epoch: 3/5... Step: 310... Loss: 2.1744... Val Loss: 2.1489\n",
      "Epoch: 3/5... Step: 320... Loss: 2.1538... Val Loss: 2.1315\n",
      "Epoch: 3/5... Step: 330... Loss: 2.1215... Val Loss: 2.1123\n",
      "Epoch: 3/5... Step: 340... Loss: 2.1417... Val Loss: 2.0944\n",
      "Epoch: 3/5... Step: 350... Loss: 2.1163... Val Loss: 2.0747\n",
      "Epoch: 3/5... Step: 360... Loss: 2.0530... Val Loss: 2.0588\n",
      "Epoch: 3/5... Step: 370... Loss: 2.0740... Val Loss: 2.0397\n",
      "Epoch: 3/5... Step: 380... Loss: 2.0627... Val Loss: 2.0249\n",
      "Epoch: 3/5... Step: 390... Loss: 2.0339... Val Loss: 2.0109\n",
      "Epoch: 3/5... Step: 400... Loss: 2.0048... Val Loss: 1.9943\n",
      "Epoch: 3/5... Step: 410... Loss: 2.0216... Val Loss: 1.9790\n",
      "Epoch: 4/5... Step: 420... Loss: 1.9988... Val Loss: 1.9663\n",
      "Epoch: 4/5... Step: 430... Loss: 1.9941... Val Loss: 1.9498\n",
      "Epoch: 4/5... Step: 440... Loss: 1.9738... Val Loss: 1.9395\n",
      "Epoch: 4/5... Step: 450... Loss: 1.9096... Val Loss: 1.9230\n",
      "Epoch: 4/5... Step: 460... Loss: 1.8962... Val Loss: 1.9103\n",
      "Epoch: 4/5... Step: 470... Loss: 1.9328... Val Loss: 1.9012\n",
      "Epoch: 4/5... Step: 480... Loss: 1.9078... Val Loss: 1.8905\n",
      "Epoch: 4/5... Step: 490... Loss: 1.9121... Val Loss: 1.8742\n",
      "Epoch: 4/5... Step: 500... Loss: 1.9122... Val Loss: 1.8610\n",
      "Epoch: 4/5... Step: 510... Loss: 1.8772... Val Loss: 1.8496\n",
      "Epoch: 4/5... Step: 520... Loss: 1.8878... Val Loss: 1.8378\n",
      "Epoch: 4/5... Step: 530... Loss: 1.8457... Val Loss: 1.8280\n",
      "Epoch: 4/5... Step: 540... Loss: 1.8167... Val Loss: 1.8188\n",
      "Epoch: 4/5... Step: 550... Loss: 1.8596... Val Loss: 1.8053\n",
      "Epoch: 5/5... Step: 560... Loss: 1.8247... Val Loss: 1.7973\n",
      "Epoch: 5/5... Step: 570... Loss: 1.8147... Val Loss: 1.7909\n",
      "Epoch: 5/5... Step: 580... Loss: 1.7864... Val Loss: 1.7778\n",
      "Epoch: 5/5... Step: 590... Loss: 1.7929... Val Loss: 1.7664\n",
      "Epoch: 5/5... Step: 600... Loss: 1.7725... Val Loss: 1.7572\n",
      "Epoch: 5/5... Step: 610... Loss: 1.7600... Val Loss: 1.7554\n",
      "Epoch: 5/5... Step: 620... Loss: 1.7654... Val Loss: 1.7438\n",
      "Epoch: 5/5... Step: 630... Loss: 1.7842... Val Loss: 1.7399\n",
      "Epoch: 5/5... Step: 640... Loss: 1.7503... Val Loss: 1.7283\n",
      "Epoch: 5/5... Step: 650... Loss: 1.7272... Val Loss: 1.7204\n",
      "Epoch: 5/5... Step: 660... Loss: 1.7101... Val Loss: 1.7132\n",
      "Epoch: 5/5... Step: 670... Loss: 1.7475... Val Loss: 1.7045\n",
      "Epoch: 5/5... Step: 680... Loss: 1.7335... Val Loss: 1.6966\n",
      "Epoch: 5/5... Step: 690... Loss: 1.7062... Val Loss: 1.6938\n"
     ]
    }
   ],
   "source": [
    "net.train()\n",
    "    \n",
    "opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# create training and validation data\n",
    "val_idx = int(len(encoded)*(1-val_frac))\n",
    "data, val_data = encoded[:val_idx], encoded[val_idx:]\n",
    "\n",
    "if(train_on_gpu):\n",
    "    net.cuda()\n",
    "\n",
    "counter = 0\n",
    "n_chars = len(net.chars)\n",
    "for e in range(epochs):\n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "\n",
    "    for x, y in get_batches(data, batch_size, seq_length):\n",
    "        counter += 1\n",
    "\n",
    "        # One-hot encode our data and make them Torch tensors\n",
    "        x = one_hot_encode(x, n_chars)\n",
    "        inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "        if(train_on_gpu):\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        # zero accumulated gradients\n",
    "        net.zero_grad()\n",
    "\n",
    "        # get the output from the model\n",
    "        output, h = net(inputs, h)\n",
    "\n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "        loss.backward()\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        opt.step()\n",
    "\n",
    "        # loss stats\n",
    "        if counter % print_every == 0:\n",
    "            # Get validation loss\n",
    "            val_h = net.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                # One-hot encode our data and make them Torch tensors\n",
    "                x = one_hot_encode(x, n_chars)\n",
    "                x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "                # Creating new variables for the hidden state, otherwise\n",
    "                # we'd backprop through the entire training history\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                inputs, targets = x, y\n",
    "                if(train_on_gpu):\n",
    "                    inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                output, val_h = net(inputs, val_h)\n",
    "                val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "            net.train() # reset to train mode after iterationg through validation data\n",
    "\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the name, for saving multiple files\n",
    "model_name = 'rnn_20_epoch.net'\n",
    "\n",
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        # tensor inputs\n",
    "        x = np.array([[net.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(net.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h])\n",
    "        # get the output of the model\n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        # get the character probabilities\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "        \n",
    "        # get top characters\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next character with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None):\n",
    "        \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annants, to teen her harse, at a mine to husbing her and at to she cell than the somathing he come, and all the counse and was anden to\n",
      "say he had to her, to that that that the convicinos of her that to har to the pospos of the celssity of she was and, and hould not\n",
      "word at the pliectess\n",
      "and seed it would\n",
      "so dingress...\n",
      "\n",
      "At he soot, but his fact,\n",
      "wish her.\n",
      "\n",
      "She wanded his some all of his hore, and wele and the\n",
      "form on\n",
      "the parsing head of she was new had alang whone had\n",
      "not her hard, and sacined, and his a mort that as the pees in the mish a contriagian of and sone was say, and had becoused\n",
      "the betine on the pitter\n",
      "on take this\n",
      "stording to the prossess and she seethed if imparsed the crost time\n",
      "the\n",
      "peasing of the mesting. He this trought of what and so see that inte to be, when he had not sitting to him, as had\n",
      "stellent the\n",
      "peeter\n",
      "the pained was a seadinc the bating,\n",
      "and a linger his had had at his foor, that he would her a sill sont of the sumpech of that he cannow the\n",
      "sontel the for one to\n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 1000, prime='Anna', top_k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Here we have loaded in a model that trained over 20 epochs `rnn_20_epoch.net`\n",
    "with open('rnn_20_epoch.net', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "    \n",
    "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
    "loaded.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And Levin said.\n",
      "\n",
      "A tears on the\n",
      "minets of\n",
      "the portally, and have his sumpersiag to the persicully with the prisce shall of\n",
      "the chish the martion and which soud and the casinor with a coms in to be soul,\n",
      "but\n",
      "she had net that she sanding her\n",
      "for his forting over. That\n",
      "he\n",
      "was not that it was till than his back of the since was no darsed, and allay houre had thing of\n",
      "thome his amare to he censided that he has him her. She was stopped taking of she had and though the\n",
      "saming at her take and a ment in him, and that the sance some on the\n",
      "sind and all of her tally had\n",
      "becoul and the mond of shile of the pronint to that the samors her and asked and the\n",
      "cealed be one sact in the room. \"In that all her, whine as her all of the same ther who care to say the court of this with of the roon, but the parsen of his confeating him\n",
      "tell, and this sement and\n",
      "center to the roals that the roon steally the mare of at one have his for she, and she seat the petition of the connice to him has hunders, he talking hould.\n",
      "\n",
      "She was say his atterting and his so camarien. The mare to his so cerplat in his hast, whet her said. And the masse had been at hid not\n",
      "samp to\n",
      "and was\n",
      "alled over alane on her that that as he asked ablicines of the printens that he had to the connouning to the counse, and stenting his fall, whice she taked the somethouth he he was shore it say\n",
      "somet over, and streing there as the came of that\n",
      "store to her, the sume to said that as te heard\n",
      "him. \"There say I'm time a lovited, with him, that he're he was that, the meatt, and to dany a mary of the\n",
      "partrest that she was sande then's fore is of have was not and her tine her aly and all the mack in whet he was not in\n",
      "offored over, there was not him\n",
      "it. So he sourd the pass, and all the most at the minter on a traigh of to sere and the crunsitat in his\n",
      "hassed,\n",
      "and souting alonice all\n",
      "the\n",
      "paces, where his for his amenes.\n",
      " \"Well, and's sene was now she had tow have it. The meations of his seetion in thit that and alone that.\n",
      "I\n",
      "went a louged there so \n"
     ]
    }
   ],
   "source": [
    "print(sample(loaded, 2000, top_k=5, prime=\"And Levin said\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

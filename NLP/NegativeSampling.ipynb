{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    # Replace punctuation with tokens so we can use them in our model\n",
    "    text = text.lower()\n",
    "    text = text.replace('.', ' <PERIOD> ')\n",
    "    text = text.replace(',', ' <COMMA> ')\n",
    "    text = text.replace('\"', ' <QUOTATION_MARK> ')\n",
    "    text = text.replace(';', ' <SEMICOLON> ')\n",
    "    text = text.replace('!', ' <EXCLAMATION_MARK> ')\n",
    "    text = text.replace('?', ' <QUESTION_MARK> ')\n",
    "    text = text.replace('(', ' <LEFT_PAREN> ')\n",
    "    text = text.replace(')', ' <RIGHT_PAREN> ')\n",
    "    text = text.replace('--', ' <HYPHENS> ')\n",
    "    text = text.replace('?', ' <QUESTION_MARK> ')\n",
    "    # text = text.replace('\\n', ' <NEW_LINE> ')\n",
    "    text = text.replace(':', ' <COLON> ')\n",
    "    words = text.split()\n",
    "    \n",
    "    # Remove all words with  5 or fewer occurences\n",
    "    word_counts = Counter(words)\n",
    "    trimmed_words = [word for word in words if word_counts[word] > 5]\n",
    "\n",
    "    return trimmed_words\n",
    "\n",
    "def create_lookup_tables(words):\n",
    "    word_counts = Counter(words)\n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True) # Sort by frequencies\n",
    "    int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
    "    vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n",
    "    \n",
    "    return vocab_to_int, int_to_vocab\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " anarchism\n"
     ]
    }
   ],
   "source": [
    "# Read in the text\n",
    "with open('data/text8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(text[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anarchism', 'originated', 'as', 'a', 'term']\n"
     ]
    }
   ],
   "source": [
    "words = preprocess(text)\n",
    "print(words[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words =  16680599\n",
      "Count of unique words =  63641\n"
     ]
    }
   ],
   "source": [
    "# Look at the data\n",
    "print(\"Total words = \", len(words))\n",
    "print(\"Count of unique words = \", len(set(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5233, 3080, 11, 5, 194, 1, 3133, 45, 58, 155]\n"
     ]
    }
   ],
   "source": [
    "# Create tranlator dictionaries words to int and int to words\n",
    "vocab_to_int, int_to_vocab = create_lookup_tables(words)\n",
    "int_words = [vocab_to_int[word] for word in words]\n",
    "print(int_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5233, 3080, 155, 127, 10571, 27349, 15067, 58112, 854, 3580, 194, 10712, 214, 1324, 454, 708, 2757, 686, 7088, 5233, 1052, 0, 320, 44611, 5233, 8983, 279, 4147, 6437, 4186]\n"
     ]
    }
   ],
   "source": [
    "# Subsampling Remove connector words like and the a etc,,,\n",
    "\n",
    "threshold = 1e-5\n",
    "word_counts = Counter(int_words)\n",
    "total_count = len(int_words)\n",
    "freqs = {word: count/total_count for word, count in word_counts.items()}\n",
    "p_drop = {word: 1 - np.sqrt(threshold/freqs[word]) for word in word_counts}\n",
    "# discard some frequent words, according to the subsampling equation\n",
    "# create a new list of words for training\n",
    "train_words = [word for word in int_words if random.random() < (1 - p_drop[word])]\n",
    "\n",
    "print(train_words[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target(words, idx, window_size = 5):\n",
    "    R = np.random.randint(1, window_size+1)\n",
    "    start = idx - R if (idx - R) > 0 else 0\n",
    "    stop = idx + R\n",
    "    target_words = words[start:idx] + words[idx + 1: stop+1]\n",
    "    return list(target_words)\n",
    "\n",
    "def get_batches(words, batch_size, window_size=5):\n",
    "    ''' Create a generator of word batches as a tuple (inputs, targets) '''\n",
    "    \n",
    "    n_batches = len(words)//batch_size\n",
    "    \n",
    "    # only full batches\n",
    "    words = words[:n_batches*batch_size]\n",
    "    \n",
    "    for idx in range(0, len(words), batch_size):\n",
    "        x, y = [], []\n",
    "        batch = words[idx:idx+batch_size]\n",
    "        for ii in range(len(batch)):\n",
    "            batch_x = batch[ii]\n",
    "            batch_y = get_target(batch, ii, window_size)\n",
    "            y.extend(batch_y)\n",
    "            x.extend([batch_x]*len(batch_y))\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[2, 3, 4, 6, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "int_text = [i for i in range(10)]\n",
    "print(int_text)\n",
    "\n",
    "idx = 5\n",
    "target = get_target(int_text, idx, window_size=5)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [0, 0, 0, 1, 1, 1, 2, 2, 2, 3]\n",
      "y\n",
      " [1, 2, 3, 0, 2, 3, 0, 1, 3, 2]\n"
     ]
    }
   ],
   "source": [
    "int_text = [i for i in range(20)]\n",
    "x,y = next(get_batches(int_text, batch_size=4, window_size=5))\n",
    "\n",
    "print('x\\n', x)\n",
    "print('y\\n', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(embedding, valid_size = 16, valid_window = 100, device = 'cpu'):\n",
    "    embed_vectors = embedding.weight\n",
    "    magnitudes = embed_vectors.pow(2).sum(dim=1).sqrt().unsqueeze(0)\n",
    "    valid_examples = np.array(random.sample(range(valid_window), valid_size//2))\n",
    "    valid_examples = np.append(valid_examples,\n",
    "                               random.sample(range(1000,1000+valid_window), valid_size//2))\n",
    "    valid_examples = torch.LongTensor(valid_examples).to(device)\n",
    "    \n",
    "    valid_vectors = embedding(valid_examples)\n",
    "    similarities = torch.mm(valid_vectors, embed_vectors.t())/magnitudes\n",
    "        \n",
    "    return valid_examples, similarities\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SkipGramNeg(nn.Module):\n",
    "    def __init__(self, n_vocab, n_embed, noise_dist=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_vocab = n_vocab\n",
    "        self.n_embed = n_embed\n",
    "        self.noise_dist = noise_dist\n",
    "        \n",
    "        # define embedding layers for input and output words\n",
    "        self.in_embed = nn.Embedding(n_vocab, n_embed)\n",
    "        self.out_embed = nn.Embedding(n_vocab, n_embed)\n",
    "        \n",
    "        # Initialize embedding tables with uniform distribution\n",
    "        # I believe this helps with convergence\n",
    "        self.in_embed.weight.data.uniform_(-1, 1)\n",
    "        self.out_embed.weight.data.uniform_(-1, 1)\n",
    "        \n",
    "    def forward_input(self, input_words):\n",
    "        input_vectors = self.in_embed(input_words)\n",
    "        return input_vectors\n",
    "    \n",
    "    def forward_output(self, output_words):\n",
    "        output_vectors = self.out_embed(output_words)\n",
    "        return output_vectors\n",
    "    \n",
    "    def forward_noise(self, batch_size, n_samples):\n",
    "        \"\"\" Generate noise vectors with shape (batch_size, n_samples, n_embed)\"\"\"\n",
    "        if self.noise_dist is None:\n",
    "            # Sample words uniformly\n",
    "            noise_dist = torch.ones(self.n_vocab)\n",
    "        else:\n",
    "            noise_dist = self.noise_dist\n",
    "            \n",
    "        # Sample words from our noise distribution\n",
    "        noise_words = torch.multinomial(noise_dist,\n",
    "                                        batch_size * n_samples,\n",
    "                                        replacement=True)\n",
    "        \n",
    "        device = \"cuda\" if model.out_embed.weight.is_cuda else \"cpu\"\n",
    "        noise_words = noise_words.to(device)\n",
    "        \n",
    "        noise_vectors = self.out_embed(noise_words).view(batch_size, n_samples, self.n_embed)\n",
    "        \n",
    "        return noise_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeSamplingLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input_vectors, output_vectors, noise_vectors):\n",
    "        \n",
    "        batch_size, embed_size = input_vectors.shape\n",
    "        \n",
    "        # Input vectors should be a batch of column vectors\n",
    "        input_vectors = input_vectors.view(batch_size, embed_size, 1)\n",
    "        \n",
    "        # Output vectors should be a batch of row vectors\n",
    "        output_vectors = output_vectors.view(batch_size, 1, embed_size)\n",
    "        \n",
    "        # bmm = batch matrix multiplication\n",
    "        # correct log-sigmoid loss\n",
    "        out_loss = torch.bmm(output_vectors, input_vectors).sigmoid().log()\n",
    "        out_loss = out_loss.squeeze()\n",
    "        \n",
    "        # incorrect log-sigmoid loss\n",
    "        noise_loss = torch.bmm(noise_vectors.neg(), input_vectors).sigmoid().log()\n",
    "        noise_loss = noise_loss.squeeze().sum(1)  # sum the losses over the sample of noise vectors\n",
    "\n",
    "        # negate and sum correct and noisy log-sigmoid losses\n",
    "        # return average batch loss\n",
    "        return -(out_loss + noise_loss).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Get our noise distribution\n",
    "# Using word frequencies calculated earlier in the notebook\n",
    "word_freqs = np.array(sorted(freqs.values(), reverse=True))\n",
    "unigram_dist = word_freqs/word_freqs.sum()\n",
    "noise_dist = torch.from_numpy(unigram_dist**(0.75)/np.sum(unigram_dist**(0.75)))\n",
    "\n",
    "# instantiating the model\n",
    "embedding_dim = 300\n",
    "model = SkipGramNeg(len(vocab_to_int), embedding_dim, noise_dist=noise_dist).to(device)\n",
    "\n",
    "# using the loss that we defined\n",
    "criterion = NegativeSamplingLoss() \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5\n",
      "Loss:  7.0021185874938965\n",
      "so | prisoner, technology, nayland, materials, sihanouk\n",
      "where | denmark, alternative, directly, distant, astronomer\n",
      "will | factor, byzantines, pomace, phenomenal, summary\n",
      "which | the, persia, outside, california, to\n",
      "the | a, of, and, in, nine\n",
      "often | per, vehicles, continues, cigarettes, the\n",
      "american | ricardo, gould, sideline, names, named\n",
      "such | him, noteworthy, coaches, wounded, focussed\n",
      "powers | tinge, exceptionally, juice, generalities, and\n",
      "rise | incest, blind, revelations, armistice, tax\n",
      "shown | transshipment, marcian, roosevelt, poles, ulvaeus\n",
      "magazine | alcoholism, correctly, strongly, translate, baptism\n",
      "hold | manufacturing, rolls, digestive, order, fdc\n",
      "grand | albertine, breach, anatolia, accelerated, more\n",
      "construction | herbs, continued, municipalities, wallace, preferring\n",
      "taking | parodies, bender, available, amphibious, essential\n",
      "...\n",
      "\n",
      "Epoch: 1/5\n",
      "Loss:  5.517529487609863\n",
      "four | two, zero, five, nine, one\n",
      "see | revision, one, a, it, the\n",
      "will | to, have, ceremony, and, in\n",
      "known | is, in, to, constitutions, a\n",
      "nine | zero, one, the, eight, two\n",
      "world | and, as, louisa, befriended, gauss\n",
      "i | occurred, in, was, there, is\n",
      "b | verona, nine, tantra, versa, population\n",
      "account | their, taller, aspects, se, floor\n",
      "versions | of, reportedly, has, rix, lineal\n",
      "issue | phylogenetics, disturbances, material, dusk, faults\n",
      "shown | marcian, transshipment, poles, fortuna, dermal\n",
      "test | immediately, mascots, vascular, automaton, state\n",
      "institute | one, stung, covers, eurasia, korea\n",
      "mainly | polluting, charing, pyrite, federalist, sincere\n",
      "existence | quarks, fortresses, plaque, jotham, routledge\n",
      "...\n",
      "\n",
      "Epoch: 1/5\n",
      "Loss:  4.67227840423584\n",
      "s | one, and, nine, zero, seven\n",
      "called | the, a, is, by, for\n",
      "there | be, have, are, that, to\n",
      "with | on, the, to, of, that\n",
      "from | of, and, one, the, in\n",
      "history | of, in, a, and, the\n",
      "war | in, the, shooting, by, abertay\n",
      "see | the, a, and, of, for\n",
      "know | killing, resonant, d, it, dylan\n",
      "operating | heat, furnish, colossus, cooperative, clicking\n",
      "versions | can, low, minimum, oscillating, stylings\n",
      "consists | shark, auc, corgan, freelance, source\n",
      "engine | against, that, berkshire, sufferer, omitted\n",
      "applied | have, assemble, bells, combustion, opined\n",
      "something | luminance, procedure, immanuel, ecology, with\n",
      "articles | kahanism, optimal, numbering, intellect, thrusters\n",
      "...\n",
      "\n",
      "Epoch: 1/5\n",
      "Loss:  3.686572551727295\n",
      "other | to, but, are, is, of\n",
      "eight | nine, seven, one, five, zero\n",
      "no | be, done, that, like, an\n",
      "new | the, in, nine, by, first\n",
      "after | in, seven, his, of, was\n",
      "be | this, are, or, it, can\n",
      "if | we, is, be, that, so\n",
      "called | a, as, the, may, with\n",
      "shows | impressionist, negligence, super, wife, xxxiv\n",
      "governor | after, president, eight, lord, abdicated\n",
      "report | th, greencine, killers, rococo, zero\n",
      "smith | pataki, in, seven, was, during\n",
      "woman | the, four, legally, has, nine\n",
      "defense | blackadder, seasoned, hera, therefore, kickoffs\n",
      "prince | one, father, eight, century, love\n",
      "assembly | national, government, members, constitution, legislative\n",
      "...\n",
      "\n",
      "Epoch: 1/5\n",
      "Loss:  3.3292572498321533\n",
      "years | population, two, four, three, life\n",
      "and | the, of, also, is, in\n",
      "if | cannot, does, each, we, will\n",
      "will | that, be, can, if, it\n",
      "war | forces, was, british, united, germany\n",
      "three | five, four, nine, six, seven\n",
      "often | a, with, has, to, more\n",
      "united | government, states, war, in, member\n",
      "ocean | sea, north, east, coast, pacific\n",
      "additional | volcanic, organisms, easily, shiloh, mobil\n",
      "account | probably, life, biblical, in, been\n",
      "quite | although, often, the, for, into\n",
      "accepted | both, meaning, become, cannot, power\n",
      "gold | silver, toto, iron, buoyant, time\n",
      "hold | piety, always, meant, his, to\n",
      "san | one, street, american, president, st\n",
      "...\n",
      "\n",
      "Epoch: 1/5\n",
      "Loss:  3.377599000930786\n",
      "in | and, the, of, was, while\n",
      "history | english, four, central, five, french\n",
      "a | and, for, has, or, the\n",
      "however | that, and, not, be, to\n",
      "often | or, as, to, and, than\n",
      "where | from, of, as, point, for\n",
      "such | and, which, as, is, a\n",
      "states | government, united, national, council, independence\n",
      "running | system, computer, program, available, development\n",
      "applied | science, entire, matter, fundamental, structure\n",
      "articles | language, dictionary, publish, oxford, web\n",
      "alternative | some, music, how, curative, visual\n",
      "something | what, interest, how, his, women\n",
      "recorded | band, album, big, series, night\n",
      "rise | western, from, tribes, around, country\n",
      "placed | more, almost, source, or, each\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_every = 1500\n",
    "steps = 0\n",
    "epochs = 5\n",
    "\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    \n",
    "    # get our input, target batches\n",
    "    for input_words, target_words in get_batches(train_words, 512):\n",
    "        steps += 1\n",
    "        inputs, targets = torch.LongTensor(input_words), torch.LongTensor(target_words)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # input, output, and noise vectors\n",
    "        input_vectors = model.forward_input(inputs)\n",
    "        output_vectors = model.forward_output(targets)\n",
    "        noise_vectors = model.forward_noise(inputs.shape[0], 5)\n",
    "\n",
    "        # negative sampling loss\n",
    "        loss = criterion(input_vectors, output_vectors, noise_vectors)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss stats\n",
    "        if steps % print_every == 0:\n",
    "            print(\"Epoch: {}/{}\".format(e+1, epochs))\n",
    "            print(\"Loss: \", loss.item()) # avg batch loss at this point in training\n",
    "            valid_examples, valid_similarities = cosine_similarity(model.in_embed, device=device)\n",
    "            _, closest_idxs = valid_similarities.topk(6)\n",
    "\n",
    "            valid_examples, closest_idxs = valid_examples.to('cpu'), closest_idxs.to('cpu')\n",
    "            for ii, valid_idx in enumerate(valid_examples):\n",
    "                closest_words = [int_to_vocab[idx.item()] for idx in closest_idxs[ii]][1:]\n",
    "                print(int_to_vocab[valid_idx.item()] + \" | \" + ', '.join(closest_words))\n",
    "            print(\"...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.in_embed.weight.to('cpu').data.numpy()\n",
    "viz_words = 380\n",
    "tsne = TSNE()\n",
    "embed_tsne = tsne.fit_transform(embeddings[:viz_words, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "for idx in range(viz_words):\n",
    "    plt.scatter(*embed_tsne[idx, :], color='steelblue')\n",
    "    plt.annotate(int_to_vocab[idx], (embed_tsne[idx, 0], embed_tsne[idx, 1]), alpha=0.7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
